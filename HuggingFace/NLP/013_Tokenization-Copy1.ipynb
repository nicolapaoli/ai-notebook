{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558877e5-e3eb-4865-866f-3151619ff146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following NLP Course here:\n",
    "# https://huggingface.co/learn/nlp-course/chapter2/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e0d142-82aa-411c-bedd-ce3092240af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Jim Henson was a puppeteer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1340f3-ac65-4c39-8e17-e81a11b26e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jim', 'Henson', 'was', 'a', 'puppeteer']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "# Limit: Vocabulary (total number of words) is too big\n",
    "tokenized_text = sentence.split()\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee20865-11d9-41b0-a1a8-a4b261675b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J',\n",
       " 'i',\n",
       " 'm',\n",
       " ' ',\n",
       " 'H',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'p',\n",
       " 'p',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'e',\n",
       " 'r']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character tokenization\n",
    "# Limit: Context (total number of tokens in a sentence) is too big\n",
    "tokenized_text = [s for s in sentence]\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b32710-8e01-4312-bbe4-f7a35adbb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subwords tokenization\n",
    "# Example with Bert\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d92c52-be14-4b68-8495-20fc54d9c954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jim', 'He', '##nson', 'was', 'a', 'puppet', '##eer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2bbf47-0690-4d36-81d4-53e1d7af9166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.tokenizers/tokenizer_config.json',\n",
       " '.tokenizers/special_tokens_map.json',\n",
       " '.tokenizers/vocab.txt',\n",
       " '.tokenizers/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\".tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea739fd-9da0-4173-b924-f26c41a80968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jim', 'He', '##nson', 'was', 'a', 'puppet', '##eer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOKENIZER Pipeline\n",
    "# 1) Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98395a41-66cd-4655-8225-2b69b3e2dca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3104, 1124, 15703, 1108, 170, 16797, 8284]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) From tokens to input IDS\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993516bc-d08f-4abc-b2ed-efafd3f45c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jim Henson was a puppeteer'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Decoding\n",
    "decoded_str = tokenizer.decode(ids)\n",
    "decoded_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8e13709-01be-4687-b7e0-a6bf32cb8d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁Jim', '▁He', 'n', 'son', '▁was', '▁', 'a', '▁puppet', 'e', 'er']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same would apply to other models:\n",
    "# 1) Tokenization\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "tokens2 = tokenizer2.tokenize(sentence)\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a988631-121a-478d-8ae1-41871bf23330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6006, 216, 29, 739, 47, 3, 9, 26141, 15, 49]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) From tokens to input IDS\n",
    "ids2 = tokenizer2.convert_tokens_to_ids(tokens2)\n",
    "ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa7700d-52ab-4e58-b745-2667931cf19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jim Henson was a puppeteer'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Decode\n",
    "tokenizer2.decode(ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d8be9-fc4d-4913-a8b3-f84d60bb2a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
